+++
date = "2017-07-19T09:50:40-07:00"
title = "Week 1: Basic Topics"
tags = ["lectures", "classification", "regression", "neural network", "first-week", "hypothesis testing", "experimental design", "basic topics"]
highlight = true
math = false
summary = """
Descriptions of the introductory data science material to be covered in the mornings of the first week. These introductory lecture topics will familiarize participants with domain-specific jargon and concepts, and serve as the starting point of a toolbox for the second-week projects.
"""

[header]
  caption = ""
  image = "posts/generic-11.jpg"

+++

# Focus

Interactive lectures on introductory topics in data science will comprise the
mronings of the first week of the workshop. The first three lectures will be
led by
[Isabell Konrad](https://www.ischool.berkeley.edu/people/isabell-konrad) and
will cover classification, regression, clustering and neural
networks. Thursday's lecture by [Yinshan Zhao](http://epims.med.ubc.ca/our-team/faculty/yinshan-zhao/) will cover hypothesis testing and experimental
design, while Friday's lecture by Michael Reid will showcase modern software
tools for data scientists. 

# Schedule

## Monday
### Linear and Logistic regression.
We will understand the first machine learning algorithm Linear Regression. We
talk about train-test-split, normalization of the data, the problem of
overfitting and regularization. A short code example shows how to use these
tools in practice (using scikit-learn). We introduce classification in contrast
to regression, and discuss key metrics like accuracy, precision and recall. We
talk about the Logistic Regression classifier and show a short code example.

## Tuesday 
### Important classifiers.
The classifiers Decision Trees and K-Nearest-Neighbours will be covered, as
well as ensemble methods like Bagging and Random Forests. In a short code
example the application of the classifiers is demonstrated (scikit-learn).

## Wednesday
### Neural Networks.
This lecture is about Neural Networks with various add-ons, as well as feature
engineering and a short code example in TensorFlow.

## Thursday
### Hypothesis testing and experimental design.
Firstly, we will discuss the purpose of hypothesis testing and learn how to do it. Through examples, we will introduce basic concepts, testing procedures, common misinterpretations and connection between hypothesis testing and estimation. Then we will discuss statistical considerations in study designs, such as sampling, controls, randomization, sample size and power. Finally, we will look into multiple testing procedures. 

## Friday
### Modern Software Tools for Data Scientists.
This lecture will cover the “plumbing” of data science — the software used for data ingestion, cleaning, storage, and analysis. We will provide an overview of the tools currently used by enterprises ingesting hundreds of thousand events per second and handling petabytes of data. Attendees will have a hands-on introduction to the machine learning tools used in industry, including distributed processing with AWS and Apache Spark. We also look into the future at the new generation of native stream processing platforms, used for real-time machine learning.
